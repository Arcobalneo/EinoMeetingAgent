{"metadata":{"description":"会议围绕大语言模型在通信领域的应用、产生幻觉的原因及解决策略，以及在移动边缘网络中部署模型的策略等进行探讨，还涉及相关案例研究和未来工作方向。","end_time":"未明确日期，01:04:14","participants":["TaoDusit","朱琨","席志远","陈家园"],"start_time":"未明确日期，00:00:19","summary":"会议中TaoDusit分享大语言模型在通信应用、幻觉问题及应对策略，还提及移动边缘网络模型部署策略和案例研究，最后解答了听众关于模型性能和应用的问题。","title":"大语言模型在通信中的应用及幻觉问题探讨会议","todo_list":["朱琨给TaoDusit发送可合作的论文"]},"raw_content":"{\"contents\":[{\"content\":{\"text\":\"Hi morning morning.\"},\"time_from\":\"00:00:19\",\"time_to\":\"00:00:35\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So I'm fixing a ticket to 19, so I I think I will fly to Shanghai first and go to Nanjing.\"},\"time_from\":\"00:00:35\",\"time_to\":\"00:00:43\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"from 上海从南京人？\"},\"time_from\":\"00:00:43\",\"time_to\":\"00:00:48\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"So I'm going with my mother.\"},\"time_from\":\"00:00:48\",\"time_to\":\"00:00:49\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Yeah, I know, I know that. So how many days will you stay?\"},\"time_from\":\"00:00:49\",\"time_to\":\"00:00:50\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"I think maybe two, two or three days.\"},\"time_from\":\"00:00:50\",\"time_to\":\"00:00:55\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"ok three days may be better。\"},\"time_from\":\"00:00:55\",\"time_to\":\"00:00:59\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah. So. I will give. I will attend one conference in Zucho after data.\"},\"time_from\":\"00:00:59\",\"time_to\":\"00:01:06\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"After that, Ok.\"},\"time_from\":\"00:01:06\",\"time_to\":\"00:01:08\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah. So maybe after on the way back to Shanghai.\"},\"time_from\":\"00:01:08\",\"time_to\":\"00:01:13\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"I see I see, so what what conference?\"},\"time_from\":\"00:01:13\",\"time_to\":\"00:01:14\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Because in the. I cannot remember, so it's a someone.\"},\"time_from\":\"00:01:14\",\"time_to\":\"00:01:17\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"In the international conference or what?\"},\"time_from\":\"00:01:17\",\"time_to\":\"00:01:21\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"No, No is a yeah more low call, but yeah, the name is international.\"},\"time_from\":\"00:01:21\",\"time_to\":\"00:01:23\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"I. C. I see, I see.\"},\"time_from\":\"00:01:23\",\"time_to\":\"00:01:29\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"So, yeah, could if there is any other paper that we can collaborate?\"},\"time_from\":\"00:01:29\",\"time_to\":\"00:01:33\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Sure, I recently just got one I will send you today.\"},\"time_from\":\"00:01:33\",\"time_to\":\"00:01:37\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Thank you. Thank you.\"},\"time_from\":\"00:01:37\",\"time_to\":\"00:01:38\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"And there should be more coming recently.\"},\"time_from\":\"00:01:38\",\"time_to\":\"00:01:44\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Ok. Ok.\"},\"time_from\":\"00:01:44\",\"time_to\":\"00:06:14\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Ok, it's turn now start. Ok. Firstly, we are very, very happy to invite Professor do it again to give us a talk of the very new and very hot topic that is the hancilation, Aware optimization for Llm Empowered Communications. We know that.\"},\"time_from\":\"00:06:14\",\"time_to\":\"00:06:19\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yep.\"},\"time_from\":\"00:06:19\",\"time_to\":\"00:06:37\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"A large language model now is so popular and it can empower the widest communication and networking and today, professor do it will introduce how L M Model can be applied for this communication network and also how to. Solve or address this hallucination aware problem in Lm powered networks, and as professor to, this is where we are too familiar, so I won't. I will not introduce him again and we'll leave the time for to do it and thank you so much for your consistent guidance and help for our group.\"},\"time_from\":\"00:06:37\",\"time_to\":\"00:07:14\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Thank you.\"},\"time_from\":\"00:07:14\",\"time_to\":\"00:07:15\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Ok, thank you very much for a very nice introduction for so in fact a large language model is just too hot. It's really hot. Even many people already say that I will be the technology of. This and makes big so Lm will be in everything there so there's a people who can see and speculate in the near future. So yeah, it might be good to look closer to the Lom for many applications and especially in networking. So so for this.\"},\"time_from\":\"00:07:15\",\"time_to\":\"00:07:56\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"I we don't want to introduce a L. L M because I think everybody know how to use it and even some of you already working on L. L. M, but we would like to look at the slightly deeper the technical and challenge. That we could have when we use Alm, especially in communication and networking, so if you are again, if you are interested in the technical detail of this talk, so the. We have a paper uploaded to archives. More importantly, we had the tutorial so the if you would like to reproduce or run the result.\"},\"time_from\":\"00:07:56\",\"time_to\":\"00:08:41\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"That we are going to present in this paper so we have the Github link that you can scan the code, the source code and data set are available in the Github website. So what we have in the tutorial is that we define a new hallucination data set for the telecom base, which is based on the telecom Q and A data set. So this is a broad general data set and then, we customize And.\"},\"time_from\":\"00:08:41\",\"time_to\":\"00:09:12\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Make a data set to be specific for the hallucination testing, so this is the open source data set and this contain 10000 items and 8000 is for training. And. 2000 is for testing sample so we can use the different algorithms, including the direct preference optimization, to reduce or mitigate the hallucination. And again the technical detail will be.\"},\"time_from\":\"00:09:12\",\"time_to\":\"00:09:44\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Presented in the letter so background about the large language model that Lm is a type of the computational model, so Lm design to perform the national language processing tasks such as the language. Generation classification, translation, conversation assistant and agent customers support Lm Capabilities has gone beyond the text domain and now they can understand multi multi model content such as images together with the text.\"},\"time_from\":\"00:09:44\",\"time_to\":\"00:10:24\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So we can look at the chat. Tplom can be widely yellow adopted in various fields now, including education, consultation, businesses, research, engineering and networking. So the key idea of the L is at the transformer and multiatented attention mechanism, so we know that the Lm is more the prediction of statistical prediction model, that is is predict.\"},\"time_from\":\"00:10:24\",\"time_to\":\"00:11:01\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Put the next words based on the previous words, so the key capability of the algorithm is that the it can link the. Features or the words from previous sequences into the future sequences, the next sequences, so does mean we can use the multi head at the engine to put the focus on the certain words that provide the most meaningful meaning. Text to the user so this multiyear attention can be integrated in the transformer block and we can have the Gpt model, so there are many applications of. So now we want to look at how the Lm can be constructed, deployed. So the life cycle includes first, the data set construction. So this is the.\"},\"time_from\":\"00:11:01\",\"time_to\":\"00:12:04\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"The third step that we have the massive and unlabeled data from the open corpora such as the. Wikipedia, so we need the Lm to be pretended so that the Lm can learn to predict the next token. Based on the patterns observed in the data set, so this training will provide the capability to capture the wide variety of linguistic noance and knowledge after a data set construction. We move on to the pretending step, so this is the supervise fight to the pretending that we use the data set to pretend the model. And then we have the supervise. So this is to align the pretend telem with the specific Dustin, for example, language translation question answer. So this is a typically done with the smaller.\"},\"time_from\":\"00:12:04\",\"time_to\":\"00:13:11\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Domain specific label data set and then we have the preference alignment, so after the. By tuning, we can further fight in the generation of the text based on the user preference. This step can be completed by the reinforcement learning with the human feedback or direct preference optimization. So in this way we are able to customize and personalize the Lm based on the user requirement.\"},\"time_from\":\"00:13:11\",\"time_to\":\"00:13:48\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"And in friends. So the after the model is has been pretty tune and online, with the user preference, we can execute the Lm to generate the content according to the prompt from the user. So now we want to look at what could be the problem that happened in Lom, especially during the inference that is the hallucination, so we might know that the.\"},\"time_from\":\"00:13:48\",\"time_to\":\"00:14:19\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"People already ask question, how can we should we trust chat Gpd or leave, so there is the problem that happened with the Alm, that is the hallucination, so hallucination refer to the effect. That the generate information that fractually incorrect or unfaithful to the provided input, so it is important to take note that hallucination.\"},\"time_from\":\"00:14:19\",\"time_to\":\"00:14:51\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Can be differed from errors, so in fact, some people say hallucination is one kind of the error, but in fact they are slightly different, so error could be just a symbol syntax or a symbol. Error, for example the English syntax or even the meaning can be error, but hallucination deeper implication because the issue is that the output may look. Real or look to be true, but the fact associated to the answer is incorrect, so this is something that we need only expert to evaluate the hallucination. While the error can be shared easily, but hallucination needs the more complex mechanism to detect, so there are different kind of the hallucination and we can broadly classify the. Hallucination into three type. So the first one is include input conflicting hallucination. This kind of hallucination happens when the Lm keep the output, which failed to adequately. Address the spec in specific input provided by the user, so look at the example on the left hand side here. The user want to know how many letter T involved in the artificial intelligence, right? So that's a letter T. So we can see that there is there are T here and T. However, that it may give the output, which is totally different from the input that the user would like to have. So if the user does not know precisely what they want, so this kind of the out.\"},\"time_from\":\"00:14:51\",\"time_to\":\"00:16:56\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"It may look to be useful, but in fact it may not. So that is some kind of the. hello 新年一下。So the second one is the fact conflicting hallucination. So this one is quite simple to detect again. The example is that we want to know how many letter T in artificial intelligence but the Lom answer that there are.\"},\"time_from\":\"00:16:56\",\"time_to\":\"00:17:21\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So this is similar to the error, but if we don't check right so we might take the output from the L L. M and use it directly so this can cause the hallucination. Another one is the context conflicting hallucination. So now do we move to the different question? For example, where are the letter letter? So yeah, I also in my answer that let's break, break down the.\"},\"time_from\":\"00:17:21\",\"time_to\":\"00:17:53\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Artificial intelligence to locate 30 so it answers that he should be here, however this one. The answer is contradicting because so there are two T letter in artificial and one in intelligence. So that is two T letter in artificial, so that's not correct when it is contradicting with the fact that the L. M already answered at the beginning. Context conflicting is similar to like we are talking one thing right and then the Lm try to answer something else so even the output form can be conflicting with each other, so that is in this example. So Ii am already say that there are two T at the different location, but later the answers say that there are two let T letters. Also is conflicting in the sense. Again, if we don't carefully check the output from the Llm so we can, we may use the output incorrectly. So this is the three kind of the hallucination now we want to move on to check. What are the reasons of the hallucination? In fact, hallucination can happen in our life stages of the. 来采购。From the training, some data set may help by as incomplete or insufficient data set collected by, for example, mobile devices.\"},\"time_from\":\"00:17:53\",\"time_to\":\"00:19:39\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Also, Lm, if trained by this bias, incomplete or insufficient data set, can generate hallucination as the response or the answer. Can be screwed from the limited corporas, so this issue is very critical for the Lom train on the diverse and unfilter content on the internet.\"},\"time_from\":\"00:19:39\",\"time_to\":\"00:20:04\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So because the content on the Internet right can be anything. So, we may not have the accurate data set or accurate information. So if we train L from the content on the Internet, hallucination can happen easily. Another thing is if we want to deploy Lom based on the mobile environment, we may train the L. Form the data that the mobile devices connected, for example, mobile so share network. So we know that.\"},\"time_from\":\"00:20:04\",\"time_to\":\"00:20:40\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Mobile device may not have a big enough data set, so this can cause the hallucination. Another problem is that. The can suffer from the model itself. So for example, if the size of the Lm is not big enough and the neural network model is not sophisticated, so in this way, the Lm may not be able to capture the data distribution and relationship.\"},\"time_from\":\"00:20:40\",\"time_to\":\"00:21:12\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"The pattern in the data set correctly and accurately, so this can cause the hallucination, especially when we deploy Lm in the mobile environment, we need to reduce the model size and that will make the degradation of the quality of the output.\"},\"time_from\":\"00:21:12\",\"time_to\":\"00:21:33\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"And the degradation can lead to the hallucination. Another issue is the user prompt. So now the hallucination can happen in the inference stage, so that means if the user does not know exactly or the input prompt is not accurate enough. It could be ineffective or ambiguous. So in this way we may, the La may fail to stay accurate and cause the hallucination, or that can be misinterpreted by the actual user demand.\"},\"time_from\":\"00:21:33\",\"time_to\":\"00:22:12\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So this is very common in the mobile interaction because the input from the mobile user can be limited. Another reason for the hallucination can happen is from the attack. So the attacker can put the back door attack to trigger the Llm so this back door can generate the undesirable output by inserting poison sample in self.\"},\"time_from\":\"00:22:12\",\"time_to\":\"00:22:42\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Data set so this hallucination attack make the result nonsensical or out of distribution prompt, composed of the random token, so in this way we are not able to detect easily whether the. Output is accurate or not, so this is the another reason for hallucination. So now we would like to see how to mitigate or reduce the hallucination in our paper. We look at two approaches. The first one is the model bed strategy.\"},\"time_from\":\"00:22:42\",\"time_to\":\"00:23:23\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So the model based strategy will utilize the technique in natural language processing to reduce the hallucination. So the first step is we would like to. Avoid hallucination because of the bad data set, so here we could in and we could implement the data set, checking to ensure that the domain specific data set is accurate and correct. So that it can avoid the hallucinatory content. So this could be used for the pretending or fight tuning later, but again, this will cause the extra step for the data set construction that is not just collecting and constructing, but we need to do the checking. If we do not check the data set carefully, some hallucinated data set can leak into the pretending and fine tuning process. Another mitigation strategy is we can try to implement the reasoning mechanism, and one of the typical way to implement reasoning in Lom is called retrieval augmented generation or so. The idea of Ra is rather simple when we have the Llm so Llm, when we execute the model, it will generate the output from the model directly. So does mean if the model. Is not trained, has not been trained by the accurate data set or it may not have a capability to produce the accurate output. For example, the number of parameters is not enough, so hallucination can happen. So one of the direct solution is.\"},\"time_from\":\"00:23:23\",\"time_to\":\"00:25:18\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Instead of relying only on the model L model parameters so we can let the Lom contact the external database, for example, we can link chat with the either. Research paper. So in that way, when we search for the output or answer regarding the wireless communication so Lm can search for the relevant document in Ie, explore a database and retrieve those.\"},\"time_from\":\"00:25:18\",\"time_to\":\"00:25:53\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Database and extract the knowledge from the document, give it to the user, so in this way we have the we can say that the Llm right, if we rely only on the model parameter, that is the solve. Output. But we can have the soft knowledge data from the external database. So this will be similar to the copy directly from the database that we have. In fact, it's not copy because I still paraps and try to combine the external datab. With the output obtained from the model itself, so another one is the prompt engineering. Again, this is involving the prompt to make it more accurate. So for example, if.\"},\"time_from\":\"00:25:53\",\"time_to\":\"00:26:47\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"The prompt the user cannot input the the model or Lm model accurately or sufficient enough so the prompt engineering can help the user to improve the input. For example, if we ask certain questions, for example, help, can you answer my question regarding the?\"},\"time_from\":\"00:26:47\",\"time_to\":\"00:27:12\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Wireless communication, so the question might be too general from engineering might interact with the user back and ask more questions. Ok, what kind of violence technology you would like to know more? So, this kind of the? The prompt engineering, which we call a field short, prompt ing or the chain of thought, can help the Lm to obtain more accurate input from the user and be able to.\"},\"time_from\":\"00:27:12\",\"time_to\":\"00:27:45\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Produce the output to with the reduce hallucination, so another approach that we look at in the our paper is the system based strategy. How can we implement or improve the system that we train or execute the Lm rather than working on the model itself?\"},\"time_from\":\"00:27:45\",\"time_to\":\"00:28:10\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So hallucination may be caused by the insufficient utilization of the computing and network resources. For example, in the mobile network, every user will maintain their own data with the different property. So if the pretending cannot recognize these users, so that means they cannot access the data. So the the training data set might be biased. So that happened especially with the non.\"},\"time_from\":\"00:28:10\",\"time_to\":\"00:28:43\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So we can reverish the technique, such as federated learning, to perform the decentralized pretending or fight to need the L. So by using the federated learning, it will encourage the privacy, preservation and more user. Will be willing to contribute the training to the so in this way we can reduce the non Iid data effect and be able to help the L obtain the better quality model.\"},\"time_from\":\"00:28:43\",\"time_to\":\"00:29:21\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Another approach is the mixture of expert, so instead of having one big model for the, we can decompose the Big Foundation model into the smaller expert model and the different expert model can be fine. Based on the specific data set, so that means the expert model can be specialized on the certain specific task or the type of the data, so here, how can the expert model reduce the hallucination? That when we receive the input from the user, so the input from the user will be rooted or sent relay to the expert, which performed the best to generate the output, so instead of executing the general model that can be affected by the hallucinated model. So we can avoid those hallucinated model parameter by director input from the user to the most. Toward the P expert to construct the output. So here the idea is based on the getting network that we have the router so the router will choose the expert model to be executed and gener. Output in the most accurate one. Another approach is that we can employ the secure multi party computing, so this architecture will enhance the security of lam training, so if we indirectly reduce the risk of hallucination, for example, we can avoid the attack.\"},\"time_from\":\"00:29:21\",\"time_to\":\"00:31:16\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"By deploying the training and inference of the L Im in the safe and secure network environment so this will enable the node within the network to jointly compute the function or the process over the input. While keeping those input private, so in this way we can keep the data private, but the training can be done by or based on those data.\"},\"time_from\":\"00:31:16\",\"time_to\":\"00:31:48\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"In this Sechio multi party computing, so does mean we create the network that is secure and safe, and in this way we can use the data set in that network environment to train the model. Avoid attack, for example, data poisoning, attack or the prompt injection attack when we execute the model. So that's the typical general approaches to avoid or to mitigate the hallucination. Now, we would like to introduce the Lom application in communication. As you know, there are many applications of.\"},\"time_from\":\"00:31:48\",\"time_to\":\"00:32:32\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Already in networking, there are quite many survey paper that discuss the different application, so we just briefly introduce some of them. So the first typical application of is the telecom oriented. So Lm can be used in many applications and also in the telecom oriented question and answer. Moreover, we can use the Lom to generate the network design, system optimization and source code.\"},\"time_from\":\"00:32:32\",\"time_to\":\"00:33:08\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Production, however, the hallucination can be the issue big issue because of the lack of domain specific knowledge. Reliance on the outdated information, limited reasoning capability and the complexity of the multi model communication content, for example, if we have the data from the.\"},\"time_from\":\"00:33:08\",\"time_to\":\"00:33:35\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Camera from the signal processing or the signal measurement together with the input text from the user. How can we combine them? So in this way the telecom specific pretending need to be done. So does mean instead of relying on the general data set.\"},\"time_from\":\"00:33:35\",\"time_to\":\"00:33:57\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"The pretending can be performed based on only the telecom data set, especially other engineering or unrelated data set, will not be used to train the Lm for the telecom oriented. An answer and the model can be even further improved by using the supervised fight tuning, so that's mean even for the Lm in communication so specific.\"},\"time_from\":\"00:33:57\",\"time_to\":\"00:34:26\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Model can be fine tuned. For example, some Lm can be specialized on cellular network, some Lm maybe specialized on Wifi or satellite communication. This could be done by the. File to link process Lm can be used for the semantic communication, so that is when the user would like to transfer the raw data. Instead of transferring the big raw data to the receiver, we can extract the semantic.\"},\"time_from\":\"00:34:26\",\"time_to\":\"00:35:01\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Information and the semantic encoder and decoder can be implemented by using the Llm here. Llm can be the global knowledge base between the. Sender and receiver so the Lm will facilitate the interpretation of the semantic information and be able to accurately obtain the semantic information to be transferred to the receiver.\"},\"time_from\":\"00:35:01\",\"time_to\":\"00:35:33\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Moreover, when we want to decode the semantic received semantic information, we can use Lom to expand and put more context detail into the output. At the receiver, again because the L cause the hallucination, for example, when we want to extract the. Semantic information at the center extracted semantic information can be hallucinated also when we receive the semantic information, and we would like to explain that semantic information to the user hallucination can also happen. Also, we can use the Lom for the optimization formulation. And so for example, we would like to optimize the wireless network resource, a location, service, provisioning and routing. So here we can give Rece. The input from the user and the Lm can generate optimization formulation in terms of the mathematical formulation, or it could be the source chord, for example, to calculate the channel to noise or the signal signal to noise, to interference and noise. Ratio. So again, when we use Lom to the provide, the network optimization, formulation and solution hallucination can happen, so it could be a.\"},\"time_from\":\"00:35:33\",\"time_to\":\"00:37:16\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"For example, in the optimization formulation, that could be the constraint that is not necessary or the constraint can be wrong, so those kind of the. Inaccurate output of the optimization formulation can be due to the hallucination in our paper. We are not going through the detail we did the survey.\"},\"time_from\":\"00:37:16\",\"time_to\":\"00:37:42\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"How the different, with references related work in the literature related to the Telecom Llm, can employ the different approaches to avoid the. Hallucination. So if you are interested in those related paper, please feel free to check our So now we would like to introduce the care study. So most of existing work on the L empowered communication, so consider the hallucination issue and adopt some method to mitigate the hallucination. However, most of them just employed model based approach or system based approach without systematically explore the effect caused by hallucination from both model and system perspective, so this is our focus, so here we.\"},\"time_from\":\"00:37:42\",\"time_to\":\"00:38:45\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Develop the telecom oriented so this hell and is in intended to answer telecom query accurately and efficiently. In particular, we try to avoid or mitigate the hallucination from two perspective. First we use the hallucination detection data set. And use the apply the direct preference optimization to make a decision whether the data set that we have is hallucinated or not.\"},\"time_from\":\"00:38:45\",\"time_to\":\"00:39:22\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Moreover, on the system side we developed the mixture of expert architecture to organize the heterogeneous large language model expert into the mobile age network. So some expert can run on the mobile devices, but because the model size might be too small, it might cost the hallucination. So in this way, if the.\"},\"time_from\":\"00:39:22\",\"time_to\":\"00:39:49\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Small model on the mobile device can cause the hallucination. Offload the large language model execution to the age server with the bigger, more sophisticated model, so in this way we can employ the. Expert model on both mobile device and so in this way we are able to choose the best.\"},\"time_from\":\"00:39:49\",\"time_to\":\"00:40:19\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Expert model. To answer the input from the user, so we construct the first step, we construct the telecom oriented hallucination data set based on the telecom question and answer general data set. So here again we have a 10000 items and we insert some of the items with the hallucination and we are able to use that.\"},\"time_from\":\"00:40:19\",\"time_to\":\"00:40:48\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Data set to test whether the large language model will be able to detect and avoid the hallucination or not. So here we use the low rank adaptation or the pretended large language model. Here it will adapt the model by introducing the trendable low rank matrices.\"},\"time_from\":\"00:40:48\",\"time_to\":\"00:41:12\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So the matrixes will modify existing work weight in the model rather rather than retrain from squash, so this will be more efficient for the. Training of the and then the next step is that we adopt the low rank adaptation language model on the hallucination data set. Here we aim to reduce the hallucination rate in response to the telecom related period, so it will operate based on the. Operate based on the binary classification mechanism, so it will adjust the probability associated with the different response and the and it will enhance the likelihood of generating the response that with the minimum hallucination. So with this algorithm, the hallucination rate of the individual large language model can be reduced. This is from the model perspective. We still need the entire system that might be affected by the other factor, for example, if we have the general model. So it might not be able to answer accurate output for the user that needs specific domain domain specific answer. So in this way, that's why we would like to employ the mixture of expert architecture. So again, the. Mixture of expert is composed of the getting mechanism, and the expert model that we employ here is fine tuned with the certain specific data set.\"},\"time_from\":\"00:41:12\",\"time_to\":\"00:43:09\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"And in this way, we are able to make the expert model specialized on a certain task. For example, one expert is specialized on answering question regarding the Wifi network design. Another expert model can be specialized on answering. The question regarding the seller system, so we show in our case study that by using both the model based, based on the Dpo, with the hallucination data set so we can improve together with the performance of the mixture of expert, again, we are not going into the detail, but roughly what we did. Here is that without the mixture of expert and the Dpo, the hallucination rate will be higher and if we employ only one of them, the hallucination rate, even it can be reduced, but still not minimum. So that's why we want to employ both together Now, I would like to move on to the second part of the talk. So this is still related to the hallucination, but we focus on the mobile age network. So that means we would like to deploy the generative Ai and large language model. But again, the limitation is that in mobile age network, the size of the model that we employ may be smaller. So this is. The concept that we want to focus on the democratized generative Ai concept so the paper is already available on Ito Explore database.\"},\"time_from\":\"00:43:09\",\"time_to\":\"00:44:54\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So we know that the generative Ai is very popular, so there are many applications and the basic model. 那么。The point here is that the generative Ai and large language model is always, and it may not be able to deploy in the mobile age devices or age server due to the limitation of the computation and energy resources. So both of.\"},\"time_from\":\"00:44:54\",\"time_to\":\"00:45:28\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"The resources are limited, so we would like to find the alternative way to reduce the computation, so the idea of the democratized generative Ai has been proposed simultaneously, but the democratized generative. Ai is more general concept because it will allow equal access so that means everybody can access especially the mobile user. It can support the personalization and customization of the generative Ai. It could provide openness and sharing.\"},\"time_from\":\"00:45:28\",\"time_to\":\"00:46:06\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Of the model and be able to support localized processing with the different platform compatibility, of course, we need to provide the privacy protection. So that's why we would like to focus on the. A small generative Ai model. When we have the small generative Ai model, we can increase the accessibility. So that's means we don't need to have the connection to the cloud data center all the time. We can execute the model locally, for example with on our mobile device or the It server. So another improvement is due to the privacy when we have the smaller model.\"},\"time_from\":\"00:46:06\",\"time_to\":\"00:46:56\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"We can deploy the model on our mobile device. So does mean we don't need to send the input. Are the output form the external network so we can reduce the privacy? We can perform it offline, so that means if we have a limited a bandwidth, for example, if we have a remote user connected to the satellite communication, we don't want to access G P T captivity through the satellite communication all the time because.\"},\"time_from\":\"00:46:56\",\"time_to\":\"00:47:32\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"It kind of cause a lot of bandwidth consumption where we have a limited bandwidth or it could be the sensor node that might not have a high speed connection to the best station, so in this way we can allow the execution of. Generative Ai and large language model in the offline mode, so that means it can execute the model on its own modile device or a local server.\"},\"time_from\":\"00:47:32\",\"time_to\":\"00:48:02\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So they are made strategy to make the model of the generative Ai smaller, so we can reduce the model size. And we are able to measure whether the model that we have. It is smaller, but how the performance is evaluated. Of course the accuracy can be reduced because the model size becomes smaller, but the question here is that if we have the smaller model, whether the output is accurate, produce the output certified.\"},\"time_from\":\"00:48:02\",\"time_to\":\"00:48:42\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"The user requirement sufficiently or not. So does mean we are not aiming to maximize the accuracy or the quality of the content because we have a smaller model size. So this is the trade off when we have a bigger model. The accuracy and the quality will be better, but if we don't have the enough resources, so whether the output is acceptable or not, so that is the trade off we would like to focus.\"},\"time_from\":\"00:48:42\",\"time_to\":\"00:49:15\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So in our paper, we look at many strategy to reduce the model size. It could be fine to Ning that we can reduce our train. Fine tune the model from the pretend model, but under the smaller model size, we can use the pruning mechanism so pruning will remove some of the parameter in the model so those parameters may not be important. Another approach is the.\"},\"time_from\":\"00:49:15\",\"time_to\":\"00:49:44\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"That is, we change the parameters from the 14 point to the Integer number. So when we execute or train the model, we may not need a lot of Gpu. We can have the knowledge distillation. So that's means we can have the big model, which is called a teacher model. So the teacher model can be used to train this smaller model, which we call student model. Another strategy is the mixture of expert where we decompose the big foundation model into the smaller model. And this smaller model will be activated selectively, so that means we don't need to activate our of them at the same time. If we have three expert model here where we we can activate only one, so that will make the.\"},\"time_from\":\"00:49:44\",\"time_to\":\"00:50:41\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Resource consumption are much smaller, so in our paper we analyze the advantages and disadvantages of the different strategy to reduce the size of the model, for example. Fine tuning. It can reduce the pretend and improve the domain adaptability. However, fine tuning still incur a significant.\"},\"time_from\":\"00:50:41\",\"time_to\":\"00:51:09\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"And also, it can risk the overfitting if we have a small data set to fine tune. So for the model pruning so it can reduce the model size, however, it can cause the accuracy loss because we remove. So the knowledge distillation, again, it can reduce the model size, but it may not be able to capture even small but important part of the model. So that's me inform the teacher teacher may not teach the student model everything and when the user or the. Student model need to produce the complex output. It may not be able to achieve that, so quantization can reduce the resource consumption. But the accuracy and the quality of the output can be suffered. Mixture of expert so it can improve the model and dynamic specialization, but it can increase the complexity.\"},\"time_from\":\"00:51:09\",\"time_to\":\"00:52:16\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Because now we need to implement the getting mechanism intelligently if the getting network is not accurately determine which expert to execute and generate the output for the user, so the quality of the Outp. Can be affected. So in order to evaluate the one Tt Tv so we check, we consider for mattresses including hallucination rate.\"},\"time_from\":\"00:52:16\",\"time_to\":\"00:52:46\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Resource consumption, generalization, accuracy and accessibility for the different mobile application. So again, we did the comparison of the key model strategy in our paper, which has explained, and. There are many application examples that you could try, so we also did the survey in our paper that different work.\"},\"time_from\":\"00:52:46\",\"time_to\":\"00:53:15\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Has been done for the wild generative Ai model. They can do the fine tuning, pruning, distillation, quantization and mixture of expert to reduce the model size while keeping the quality of the output acceptable. So here we would like to look at how the.\"},\"time_from\":\"00:53:15\",\"time_to\":\"00:53:39\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Network application can be used with the compact or reduce the model strategy so we can look at the. Cloud Edge Collaboration network that is instead of just directly reduce the model size. We can have the different model size deployed at the different stage. Some smaller one can be deployed on the mobile device.\"},\"time_from\":\"00:53:39\",\"time_to\":\"00:54:06\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Some bigger can be deployed at the edge server and the full scale model can be employed at the cloud so if they can work collaboratively so the output. Can be high quality and the performance can be better. Also, we can adapt the adopt the multi agent mechanism, so that means we can have multi agent working together to make a decision to generate the output, so that means instead of relying on the big. Foundation model, we can decompose the model into the different agent, and then we let the agent interact with each other. Key multi agent is different from the mixture of expert because the mixture of experts we decompose the model and then we execute the model directly. On the other hand, multi agent is similar to the human they can interact. They can root the output from one person, one agent to another agent. So that's mean one agent may provide a preliminary and another agent can provide deeper or expanded. Output for the user and another approach is optimization or for the different network, so that means we can employ offloading on the networking resource adaptability to improve the models model model size.\"},\"time_from\":\"00:54:06\",\"time_to\":\"00:55:41\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So we conduct the case study. So the idea here is that we perform the model optimization by using the distillation pruning quantization, and then we make a decision of offloading whether we should offload the. Input tasks from the user to the Edge server or not in our case study, use the large language model as the example to evaluate the performance. And we deploy and evaluate the performance. So in our case study, we show that the different approaches based on the the violence. Large language model can result in the different result accuracy, hallucination rate accessibility and the resource consumption so.\"},\"time_from\":\"00:55:41\",\"time_to\":\"00:56:38\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"By using these multiple mattresses, we are able to adjust the model size reduction strategy, for example, how many parameters we want to put or how many experts we want to employ. In the head network age computing network, so these are the consideration. So we did the experiment and we show that we can adjust.\"},\"time_from\":\"00:56:38\",\"time_to\":\"00:57:05\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"It is similar to the multi objective function that we can adjust the model size and also the different strategies so that we can peak. There are certain performance metrics, but of course some other performance metrics will be degraded. For example, if we want to have a high accuracy, Maybe we need more resource consumption or the model size will be. So these are the trade offs that we need to optimize. So there are many future work that we need to look at, for example, when we make the model size smaller and deploying the. Edge server or mobile devices, security and privacy need to be taken into account the networking issue, so if we would like to offload the small and large model generative Ai model. We need to have the accurate and is enough network resources. Also, how could we perform the optimization? So we need to formulate the optimization for a problem that how many model size that we want to reduce?\"},\"time_from\":\"00:57:05\",\"time_to\":\"00:58:19\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So that's all for my talk today. And if you have any question, thank you very much.\"},\"time_from\":\"00:58:19\",\"time_to\":\"00:58:25\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Thank you so much. 那你送吧。So any questions from the audience?\"},\"time_from\":\"00:58:25\",\"time_to\":\"00:58:27\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah, you're welcome. Yes, please.\"},\"time_from\":\"00:58:27\",\"time_to\":\"00:58:37\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Let me check the audience. I may choose someone to. OK 习志远有没有问题，智远？\"},\"time_from\":\"00:58:37\",\"time_to\":\"00:58:48\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"听得到吗朱老师？\"},\"time_from\":\"00:58:48\",\"time_to\":\"00:58:51\",\"user\":\"席志远\"},{\"content\":{\"text\":\"可以可以。\"},\"time_from\":\"00:58:51\",\"time_to\":\"00:58:52\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Considering the constraints of mobile age deployments, how do you manage the trade off between reducing hallucinations and maintaining low service? Latency and energy costs.\"},\"time_from\":\"00:58:52\",\"time_to\":\"00:59:07\",\"user\":\"席志远\"},{\"content\":{\"text\":\"Yeah, thank you very much for the question. So the idea here is that you can see that there are different, different rectangle here that represent the different method. For example, if we see that we would like to use this llama model. So the performance is very good here. However, this model is so big.\"},\"time_from\":\"00:59:07\",\"time_to\":\"00:59:30\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"But the accessibility may not be so good. So in the instead of deploying the big model, we can use a smaller model and we can see that. When we have the smaller model, the size of the model here is smaller, but it may cause the hallucination rate to be higher. So in this way we can make a thread of which one we would like to select that. Say it could be strategy of a green or it could be the orange or the blue.\"},\"time_from\":\"00:59:30\",\"time_to\":\"01:00:06\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"That we would like to employ, so it is a multi objective optimization.\"},\"time_from\":\"01:00:06\",\"time_to\":\"01:00:15\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Ok, Thank you. I got it.\"},\"time_from\":\"01:00:15\",\"time_to\":\"01:00:16\",\"user\":\"席志远\"},{\"content\":{\"text\":\"You're welcome.\"},\"time_from\":\"01:00:16\",\"time_to\":\"01:00:19\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Any other questions.\"},\"time_from\":\"01:00:19\",\"time_to\":\"01:00:21\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Any questions, please?\"},\"time_from\":\"01:00:21\",\"time_to\":\"01:00:24\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"陈家园有没有什么问题？\"},\"time_from\":\"01:00:24\",\"time_to\":\"01:00:33\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"So yeah, after this talk, yeah, if you have any further question, yeah, please feel free to contact me or the first order of the paper.\"},\"time_from\":\"01:00:33\",\"time_to\":\"01:00:46\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Ok, actually, I have one more question. I actually, I, I'm I'm I'm very interested in the application of the large language model for the optimization problem formulation. So how is the flexibility now for for?\"},\"time_from\":\"01:00:46\",\"time_to\":\"01:00:48\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah, yeah, please. Yeah. Yeah, yeah, yeah.\"},\"time_from\":\"01:00:48\",\"time_to\":\"01:01:02\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Use Lm to formulate mathematically a real or practical wireless communication optimization problem. So does anyone did that now or has anyone done what now?\"},\"time_from\":\"01:01:02\",\"time_to\":\"01:01:12\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Sorry, yeah, Professor Kona. In fact, we yeah, we did this like almost two years ago. So yeah, the first paper that we employ. And. So we try to formulate the the the network being for main problems. So that mean we have the database. And then we use the language model to retrieve based on the input from the user to retrieve the.\"},\"time_from\":\"01:01:12\",\"time_to\":\"01:01:43\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Optimization formulation from those research paper and even combine them to formulate the new problems, but again, so so because we don't have a. Big data set so we can demonstrate that it can be done, but I'm sure that many other researchers already take this approach to develop more sophisticated and more capable large language model to formulate the optimization.\"},\"time_from\":\"01:01:43\",\"time_to\":\"01:02:10\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"So does the Ip Explorer provide the Api for the interface we we done? So we download the papers by ourselves and we feed them with the sensing.\"},\"time_from\":\"01:02:10\",\"time_to\":\"01:02:14\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"No, No, No. Yeah, manually. In fact, yeah, this is quite illegal because when we download the paper right, the copyright issue can happen.\"},\"time_from\":\"01:02:14\",\"time_to\":\"01:02:30\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"粤语。\"},\"time_from\":\"01:02:30\",\"time_to\":\"01:02:33\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah, we actually we have. We have the limit save per person as maybe 100 or papers per day from the.\"},\"time_from\":\"01:02:33\",\"time_to\":\"01:02:40\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah, and in fact we cannot use the I mean for education is Ok, but if we want to produce the language model, that I mean commercializer.\"},\"time_from\":\"01:02:40\",\"time_to\":\"01:02:43\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"好爷爷。I see.\"},\"time_from\":\"01:02:43\",\"time_to\":\"01:02:52\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"So we cannot do that, because the database of prohibits that.\"},\"time_from\":\"01:02:52\",\"time_to\":\"01:02:53\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Yeah, yeah. So actually, I'm thinking. Or maybe it's interesting to investigate what might be the minimum number of papers in this specific area that can fit into the model that they can generate some reasonable. And formulation. 对。\"},\"time_from\":\"01:02:53\",\"time_to\":\"01:03:15\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah, yeah, yeah, that's that could be the. Analysis that can be done.\"},\"time_from\":\"01:03:15\",\"time_to\":\"01:03:20\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Yeah, yeah, yeah, in that case, I. Maybe the students can use that right to graduate. 呵呵。So any other questions from the?\"},\"time_from\":\"01:03:20\",\"time_to\":\"01:03:25\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah, so now we don't have any work to do.\"},\"time_from\":\"01:03:25\",\"time_to\":\"01:03:40\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"If you know that we thank Professor, do it again for this wonderful talk and it gives so many inspirations for the future research directions and I guess most audience will learn a lot about this talk. Thank you so much. And not for your music.\"},\"time_from\":\"01:03:40\",\"time_to\":\"01:03:54\",\"user\":\"朱琨\"},{\"content\":{\"text\":\"Yeah. Thank you. Thank you very much. So look forward to meeting you in person in L. Ok, Bye bye. Thank you. Thank you.\"},\"time_from\":\"01:03:54\",\"time_to\":\"01:04:04\",\"user\":\"TaoDusit\"},{\"content\":{\"text\":\"Good. Good bye. Thank you, bye.\"},\"time_from\":\"01:04:04\",\"time_to\":\"01:04:14\",\"user\":\"朱琨\"}]}"}